{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaffyBoss/Plant-Disease-Image-Classifier/blob/main/PlantCare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJsxi5G-DED6",
        "outputId": "aad500e0-1672-49ac-bce8-cbb6b0e52f9c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Dataset URL: https://www.kaggle.com/datasets/emmarex/plantdisease\n",
            "License(s): unknown\n",
            "Downloading plantdisease.zip to plant_disease\n",
            " 97% 640M/658M [00:06<00:00, 152MB/s]\n",
            "100% 658M/658M [00:06<00:00, 112MB/s]\n",
            "Extracting plant_disease/plantdisease.zip...\n",
            "Archive:  plantdisease.zip\n",
            "replace PlantVillage/Pepper__bell___Bacterial_spot/0022d6b7-d47c-4ee2-ae9a-392a53f48647___JR_B.Spot 8964.JPG? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "\n",
        "# Upload your kaggle.json API token file by clicking the folder icon on the left -> Upload\n",
        "# Alternatively, use Colab's secrets management for security.\n",
        "# Add your Kaggle API key to the secrets manager under the \"ðŸ”‘\" icon on the left.\n",
        "# Name the secret 'KAGGLE_KEY' and paste the content of your kaggle.json file there.\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the Kaggle API key from Colab secrets\n",
        "# Ensure you have added 'KAGGLE_KEY' to Colab secrets\n",
        "kaggle_key = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "# Create the ~/.kaggle directory and kaggle.json file\n",
        "!mkdir -p ~/.kaggle\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    f.write(kaggle_key)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset - specifying the destination directory\n",
        "!kaggle datasets download -d emmarex/plantdisease -p plant_disease\n",
        "\n",
        "# Unzip the dataset into the specified directory and remove the zip file\n",
        "# We need to ensure the directory exists and the zip file is there before attempting to unzip\n",
        "import time\n",
        "time.sleep(5) # Add a small delay\n",
        "\n",
        "dataset_zip_path = 'plant_disease/plantdisease.zip'\n",
        "dataset_extract_path = 'plant_disease/'\n",
        "\n",
        "if os.path.exists(dataset_zip_path):\n",
        "    print(f\"Extracting {dataset_zip_path}...\")\n",
        "    !cd plant_disease && unzip plantdisease.zip && rm plantdisease.zip\n",
        "    print(f\"Contents of {dataset_extract_path} after unzip:\")\n",
        "    !ls plant_disease\n",
        "else:\n",
        "    print(f\"Error: {dataset_zip_path} not found. Dataset download might have failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mZNMpnQsektP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# Add a small delay to ensure files are accessible\n",
        "time.sleep(2)\n",
        "\n",
        "# Assuming the image directories are directly inside 'plant_disease'\n",
        "data_dir = 'plant_disease'\n",
        "\n",
        "# Verify the data_dir exists\n",
        "if os.path.exists(data_dir):\n",
        "    print(f\"\\nContents of {data_dir}:\")\n",
        "    print(os.listdir(data_dir))\n",
        "else:\n",
        "    print(f\"\\nError: The directory '{data_dir}' was not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jXroX3ueeyjy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o63bTiTafmha"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "\n",
        "# Create separate data generators for training, validation, and testing\n",
        "# Use 80% for training, 10% for validation, and 10% for testing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    validation_split=0.2,  # 20% data for validation and test\n",
        "    rescale=1./255,        # Normalize pixel values from 0-255 to 0-1\n",
        "    rotation_range=40,     # Data augmentation: rotate images\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Create a separate generator for the test set without augmentation\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    subset='training', # Use the training subset (80%)\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create a new ImageDataGenerator for validation with a different validation split\n",
        "validation_datagen = ImageDataGenerator(validation_split=0.1)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    subset='validation', # Use the validation subset (10% of the remaining 20%)\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create the test generator using the test_datagen\n",
        "# This will use the other 10% of the data\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    subset='validation', # Use the validation subset to get the remaining 10%\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "\n",
        "# Adjust the number of steps per epoch for training and validation\n",
        "steps_per_epoch = train_generator.samples // batch_size\n",
        "validation_steps = validation_generator.samples // batch_size\n",
        "test_steps = test_generator.samples // batch_size\n",
        "\n",
        "\n",
        "print(\"Number of steps per epoch for training:\", steps_per_epoch)\n",
        "print(\"Number of steps for validation:\", validation_steps)\n",
        "print(\"Number of steps for testing:\", test_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MarR4U6Nf7Kw"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rQB8yg2EjVwC"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cR015DAuji_c"
      },
      "outputs": [],
      "source": [
        "model.save('plant_disease_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b9TpH9HUj9qn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Upload your test image here in Colab by clicking folder icon -> Upload\n",
        "# Replace 'your_test_leaf.jpg' with the actual filename of your uploaded image.\n",
        "img_path = '/content/Your_test_leaf.jpg'\n",
        "\n",
        "# Check if the file exists before attempting to load\n",
        "if not os.path.exists(img_path):\n",
        "    print(f\"Error: The file '{img_path}' was not found.\")\n",
        "    print(\"Please upload your test image to the Colab environment and update 'img_path' with the correct filename.\")\n",
        "else:\n",
        "    img = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    pred = model.predict(img_array)\n",
        "    predicted_class = train_generator.class_indices\n",
        "    # Invert the dictionary to get class names from indices\n",
        "    class_names = {v: k for k, v in predicted_class.items()}\n",
        "    print(\"Predicted disease class:\", class_names[np.argmax(pred)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3e6017b"
      },
      "source": [
        "# Task\n",
        "Generate a complete README file in markdown format for the Plant Disease Classification project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44862ff0"
      },
      "source": [
        "## Project title and description\n",
        "\n",
        "### Subtask:\n",
        "Add a clear title and a brief description of the project (Plant Disease Classification).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3f9f11e"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a markdown file and add the title and description as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "61af6dc9"
      },
      "outputs": [],
      "source": [
        "%%writefile README.md\n",
        "# Plant Disease Classification\n",
        "\n",
        "This project aims to classify different plant diseases from images using a Convolutional Neural Network (CNN). The model is trained on a dataset of plant leaf images, categorized by plant type and disease. The goal is to provide an automated system for identifying plant diseases, which can aid in early detection and treatment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4013691"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Subtask:\n",
        "Explain how to set up the environment, including installing necessary libraries and downloading the dataset using Kaggle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d841395"
      },
      "source": [
        "**Reasoning**:\n",
        "Append the setup instructions to the README.md file, including library installations, Kaggle setup, and data download steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "44402a5c"
      },
      "outputs": [],
      "source": [
        "%%writefile -a README.md\n",
        "\n",
        "## Setup\n",
        "\n",
        "To run this project, you need to set up your environment by installing the necessary libraries and downloading the dataset.\n",
        "\n",
        "### 1. Install Libraries\n",
        "\n",
        "Install the required Python libraries using pip:\n",
        "bash\n",
        "pip install kaggle tensorflow matplotlib\n",
        "\n",
        "### 2. Kaggle API Setup\n",
        "\n",
        "To download the dataset, you need a Kaggle account and an API token.\n",
        "\n",
        "*   Go to your Kaggle account settings (`https://www.kaggle.com/<username>/account`).\n",
        "*   Under the \"API\" section, click \"Create New API Token\". This will download a `kaggle.json` file.\n",
        "*   Upload this `kaggle.json` file to your working environment (e.g., Google Colab or your local project directory).\n",
        "*   Ensure the file is placed in a directory named `.kaggle` in your home directory (`~/.kaggle/kaggle.json`). If you're using Google Colab, the provided notebook cell handles this automatically after uploading the file.\n",
        "*   Set the appropriate file permissions:\n",
        "bash\n",
        "mkdir -p ~/.kaggle\n",
        "cp kaggle.json ~/.kaggle/\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "### 3. Download and Extract Dataset\n",
        "\n",
        "Download the dataset using the Kaggle API command and extract it:\n",
        "bash\n",
        "kaggle datasets download -d emmarex/plantdisease\n",
        "unzip plantdisease.zip -d plant_disease\n",
        "\n",
        "This will download the dataset to a file named `plantdisease.zip` and extract its contents into a directory named `plant_disease`. The relevant image data is located in the `plant_disease/PlantVillage` subdirectory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a42fe23b"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "### Subtask:\n",
        "Provide details about the dataset used, including its source and structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b962dad"
      },
      "source": [
        "**Reasoning**:\n",
        "Append the \"Dataset\" section to the README.md file, including the source, structure, and data counts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5262406c"
      },
      "outputs": [],
      "source": [
        "%%writefile -a README.md\n",
        "\n",
        "## Dataset\n",
        "\n",
        "The dataset used for this project is the \"Plant Disease Classification\" dataset available on Kaggle, provided by user \"emmarex\".\n",
        "\n",
        "The dataset consists of a large collection of plant leaf images. It is organized into directories where each directory name corresponds to a specific plant species and disease (or healthy state). This structure allows for easy loading and categorization of images for training and validation.\n",
        "\n",
        "As loaded, the dataset contains images belonging to 15 different classes. The data was split into training and validation sets using an 80/20 ratio.\n",
        "\n",
        "*   **Training Images:** 16516\n",
        "*   **Validation Images:** 4122"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9676172a"
      },
      "source": [
        "## Model architecture\n",
        "\n",
        "### Subtask:\n",
        "Describe the CNN model architecture used for classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "349e9609"
      },
      "source": [
        "**Reasoning**:\n",
        "Append the \"Model Architecture\" section to the README file, describing the CNN layers and their configurations based on the model summary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9ed52462"
      },
      "outputs": [],
      "source": [
        "%%writefile -a README.md\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "The classification model is a Convolutional Neural Network (CNN) built using TensorFlow and Keras. The architecture consists of the following layers:\n",
        "\n",
        "1.  **Conv2D Layer:** 32 filters, 3x3 kernel size, ReLU activation. Input shape is (180, 180, 3) for the first layer.\n",
        "2.  **MaxPooling2D Layer:** 2x2 pool size. Reduces spatial dimensions.\n",
        "3.  **Conv2D Layer:** 64 filters, 3x3 kernel size, ReLU activation.\n",
        "4.  **MaxPooling2D Layer:** 2x2 pool size.\n",
        "5.  **Conv2D Layer:** 128 filters, 3x3 kernel size, ReLU activation.\n",
        "6.  **MaxPooling2D Layer:** 2x2 pool size.\n",
        "7.  **Flatten Layer:** Flattens the output from the convolutional layers into a 1D vector.\n",
        "8.  **Dense Layer:** 512 neurons, ReLU activation. A fully connected layer.\n",
        "9.  **Dense Layer:** Number of neurons equals the number of classes (15), Softmax activation. This output layer provides the probability distribution over the plant disease classes.\n",
        "\n",
        "The model is compiled using the Adam optimizer and categorical crossentropy loss, with accuracy as the evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5b88353e"
      },
      "outputs": [],
      "source": [
        "epochs = 20 # Increased epochs\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4c25388c"
      },
      "outputs": [],
      "source": [
        "%%writefile -a README.md\n",
        "\n",
        "## Model Evaluation\n",
        "\n",
        "After training, the model was evaluated on a separate test set to assess its performance on unseen data.\n",
        "\n",
        "*   **Test Loss:** {{loss:.4f}}\n",
        "*   **Test Accuracy:** {{accuracy:.4f}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e162e80a"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a30d2706"
      },
      "outputs": [],
      "source": [
        "%%writefile -a README.md\n",
        "\n",
        "## Sample Prediction Results\n",
        "\n",
        "Below are the prediction results for the sample images used:\n",
        "\n",
        "*   `Your_test_leaf.jpg`: Predicted disease class: Tomato_Late_blight\n",
        "*   `OIP.webp`: Predicted disease class: Pepper__bell___Bacterial_spot\n",
        "*   `Spinach-leaf-spot-disease.jpg`: Predicted disease class: Pepper__bell___Bacterial_spot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ef7d36f3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# List of uploaded images you want to classify.\n",
        "# Make sure these files are uploaded to your Colab environment.\n",
        "image_list = ['/content/Your_test_leaf.jpg', '/content/OIP.webp', '/content/Spinach-leaf-spot-disease.jpg']\n",
        "\n",
        "# Assuming img_height and img_width are already defined from previous cells\n",
        "# Assuming model and train_generator are already defined from previous cells\n",
        "\n",
        "# Invert the dictionary to get class names from indices\n",
        "class_names = {v: k for k, v in train_generator.class_indices.items()}\n",
        "\n",
        "for img_path in image_list:\n",
        "    # Check if the file exists before attempting to load\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"Error: The file '{img_path}' was not found.\")\n",
        "        print(f\"Please upload '{img_path}' to the Colab environment.\")\n",
        "    else:\n",
        "        img = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "        img_array = image.img_to_array(img) / 255.0\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "        pred = model.predict(img_array)\n",
        "        predicted_class_index = np.argmax(pred)\n",
        "\n",
        "        print(f\"Predicted disease for {os.path.basename(img_path)}:\", class_names[predicted_class_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b02d552f"
      },
      "source": [
        "# Task\n",
        "Update the README with a section on how to make predictions with the trained model, including the prediction code and the results for the sample images. Also, modify the data loading process to create a separate test set, train the model for more epochs, evaluate the model on the test set, and update the README with the evaluation results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91b1307b"
      },
      "source": [
        "## Update readme with prediction section\n",
        "\n",
        "### Subtask:\n",
        "Append a section to the README explaining how to make predictions with the trained model and include the prediction code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49ec9fd2"
      },
      "source": [
        "**Reasoning**:\n",
        "Append the \"Making Predictions\" section to the README.md file, including instructions and the prediction code snippet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ffe49df3"
      },
      "outputs": [],
      "source": [
        "%%writefile -a README.md\n",
        "\n",
        "## Making Predictions\n",
        "\n",
        "Once the model is trained, you can use it to predict the disease of a new plant leaf image.\n",
        "\n",
        "1.  **Upload your test image:** Upload the image file you want to classify to your working environment (e.g., the Colab session or your local project directory).\n",
        "2.  **Update the image path:** Replace `'path/to/your/test_image.jpg'` in the code below with the actual path to your uploaded image.\n",
        "3.  **Run the prediction code:** Execute the following Python code:\n",
        "python\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Assuming img_height and img_width are defined (e.g., 180)\n",
        "# Assuming the trained 'model' is loaded\n",
        "# Assuming 'train_generator' is available to get class names\n",
        "\n",
        "# Replace with the path to your test image\n",
        "img_path = 'path/to/your/test_image.jpg'\n",
        "\n",
        "# Check if the file exists before attempting to load\n",
        "if not os.path.exists(img_path):\n",
        "    print(f\"Error: The file '{img_path}' was not found.\")\n",
        "else:\n",
        "    img = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    pred = model.predict(img_array)\n",
        "    # Invert the dictionary to get class names from indices\n",
        "    class_names = {v: k for k, v in train_generator.class_indices.items()}\n",
        "    predicted_class_index = np.argmax(pred)\n",
        "    print(f\"Predicted disease for {os.path.basename(img_path)}:\", class_names[predicted_class_index])\n",
        "\n",
        "The output will show the predicted disease class for the image you provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29133faf"
      },
      "source": [
        "## Add deployment instructions to readme\n",
        "\n",
        "### Subtask:\n",
        "Create a new section in the README outlining potential methods for deploying the trained model (e.g., using TensorFlow Lite, TensorFlow.js, or a web framework)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e546f28b"
      },
      "source": [
        "**Reasoning**:\n",
        "Append a markdown section to the README file with potential deployment options for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4e85d6b5"
      },
      "outputs": [],
      "source": [
        "%%writefile -a README.md\n",
        "\n",
        "## Deployment\n",
        "\n",
        "There are several ways to deploy this trained plant disease classification model for practical use:\n",
        "\n",
        "*   **TensorFlow Lite:** Convert the model to TensorFlow Lite format (`.tflite`) for deployment on mobile and edge devices (Android, iOS, Raspberry Pi, etc.). This is suitable for on-device inference.\n",
        "*   **TensorFlow.js:** Convert the model to TensorFlow.js format for deployment in web browsers. This allows for running predictions directly in a web application without a backend server.\n",
        "*   **Web Application:** Build a web application using frameworks like Flask or Django (Python), or Node.js (JavaScript). The application can accept image uploads and use the saved model (`plant_disease_model.h5`) to make predictions on the server-side.\n",
        "*   **Cloud Platforms:** Deploy the model on cloud platforms like Google Cloud AI Platform, AWS SageMaker, or Azure Machine Learning. These platforms offer scalable solutions for hosting and serving machine learning models.\n",
        "\n",
        "The choice of deployment method depends on the target environment and desired user experience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2bb3e38"
      },
      "source": [
        "## Add prediction results to readme\n",
        "\n",
        "### Subtask:\n",
        "Include the results of the predictions for the sample images in the README.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c00d057c"
      },
      "source": [
        "**Reasoning**:\n",
        "Append a markdown section to the README file including the sample image filenames and their corresponding predicted classes obtained from the previous code execution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dcc74626"
      },
      "outputs": [],
      "source": [
        "%%writefile -a README.md\n",
        "\n",
        "## Sample Prediction Results\n",
        "\n",
        "Below are the prediction results for the sample images used:\n",
        "\n",
        "*   `Your_test_leaf.jpg`: Predicted disease class: Tomato_Late_blight\n",
        "*   `OIP.webp`: Predicted disease class: Pepper__bell___Bacterial_spot\n",
        "*   `Spinach-leaf-spot-disease.jpg`: Predicted disease class: Pepper__bell___Bacterial_spot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0e34390c"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "\n",
        "# Print the test results\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6ec78d3"
      },
      "source": [
        "# Task\n",
        "Update the README with future improvements, deployment instructions, and suggestions for model performance improvement strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ae11ee4"
      },
      "source": [
        "## Update readme with future improvements\n",
        "\n",
        "### Subtask:\n",
        "Add a section to the README suggesting potential areas for future work and improvements to the model and project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69475c3a"
      },
      "source": [
        "**Reasoning**:\n",
        "Append the \"Future Improvements\" section to the README.md file, listing potential enhancements for the project as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "69d0b620"
      },
      "outputs": [],
      "source": [
        "%%writefile -a README.md\n",
        "\n",
        "## Future Improvements\n",
        "\n",
        "This project can be further improved and expanded in several ways:\n",
        "\n",
        "*   **More Diverse Dataset:** Incorporate additional datasets to include a wider variety of plant diseases and species, making the model more generalized.\n",
        "*   **Advanced Data Augmentation:** Implement more sophisticated data augmentation techniques to increase the training data variability and improve model robustness.\n",
        "*   **Explore Different Architectures:** Experiment with state-of-the-art CNN architectures (e.g., pre-trained models like ResNet, Inception, EfficientNet) which might offer better performance and efficiency.\n",
        "*   **User-Friendly Interface:** Develop a web or mobile application that allows users to easily upload plant leaf images and receive disease predictions.\n",
        "*   **Model Interpretability:** Investigate techniques to understand which features or parts of the image the model focuses on for its predictions.\n",
        "*   **Real-time Detection:** Explore methods for real-time or near-real-time plant disease detection, potentially using optimized models or edge computing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNN++kLl/7JHQb6Ofb/prnz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}